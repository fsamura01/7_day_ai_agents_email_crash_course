{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Day 5: Evaluation Results\n",
    "\n",
    "This notebook provides an interactive view of the agent's performance against synthetic test cases. The evaluations are generated by a **Llama 3.3 70B** model acting as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 evaluation records.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 1. Load the evaluation results generated by evaluate_batch.py\n",
    "try:\n",
    "    with open('evaluation_results.json', 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"Successfully loaded {len(results)} evaluation records.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: evaluation_results.json not found. Please run 'python evaluate_batch.py' first.\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Results Summary Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "      <th>Verdict Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>What is the mechanism used by the API to preve...</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ûñ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>The agent's response is of high quality, follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>What is the specific comparison method used in...</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚ûñ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>The agent's response is accurate, relevant, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                           Question  \\\n",
       "0  test_1  What is the mechanism used by the API to preve...   \n",
       "1  test_2  What is the specific comparison method used in...   \n",
       "\n",
       "  instructions_follow answer_relevant answer_clear answer_citations  \\\n",
       "0                   ‚úÖ               ‚úÖ            ‚úÖ                ‚ûñ   \n",
       "1                   ‚úÖ               ‚úÖ            ‚úÖ                ‚ûñ   \n",
       "\n",
       "  completeness tool_call_search  \\\n",
       "0            ‚úÖ                ‚úÖ   \n",
       "1            ‚úÖ                ‚úÖ   \n",
       "\n",
       "                                     Verdict Summary  \n",
       "0  The agent's response is of high quality, follo...  \n",
       "1  The agent's response is accurate, relevant, an...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Flatten data for a clean table view\n",
    "display_data = []\n",
    "for res in results:\n",
    "    row = {\n",
    "        \"ID\": res.get('test_case_id'),\n",
    "        \"Question\": res.get('question'),\n",
    "        \"Verdict Summary\": res.get('evaluation', {}).get('summary', '')\n",
    "    }\n",
    "    \n",
    "    # Add individual checks with icons\n",
    "    checklist = res.get('evaluation', {}).get('checklist', [])\n",
    "    for check in checklist:\n",
    "        name = check.get('check_name')\n",
    "        val = check.get('check_pass')\n",
    "        if val is True: icon = \"‚úÖ\"\n",
    "        elif val is False: icon = \"‚ùå\"\n",
    "        else: icon = \"‚ûñ\" # Null/Unknown\n",
    "        row[name] = icon\n",
    "        \n",
    "    display_data.append(row)\n",
    "\n",
    "if display_data:\n",
    "    df = pd.DataFrame(display_data)\n",
    "    # Reorder columns to put interesting metrics first\n",
    "    cols = [\"ID\", \"Question\"] + [c for c in df.columns if c not in [\"ID\", \"Question\", \"Verdict Summary\"]] + [\"Verdict Summary\"]\n",
    "    display(df[cols])\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Deep Dive: Justifications\n",
    "Review why the judge made certain decisions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Test test_1: What is the mechanism used by the API to prevent conflicting task updates?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Agent Response:**\n",
       "> The API uses optimistic concurrency control to prevent conflicting task updates. This is implemented through the ability to update tasks, where the API checks for any potential conflicts based on timestamps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Check | Result | Justification |\n",
       "| :--- | :--- | :--- |\n",
       "| instructions_follow | ‚úÖ PASS | The agent followed its system instructions by using the 'text_search' tool to look up information before answering. |\n",
       "| answer_relevant | ‚úÖ PASS | The response directly answers the user's specific question about the mechanism used by the API to prevent conflicting task updates. |\n",
       "| answer_clear | ‚úÖ PASS | The answer is easy to understand and technically accurate, explaining the use of optimistic concurrency control and timestamp-based conflict checks. |\n",
       "| answer_citations | ‚ûñ SKIP | There are no source files to cite in this response. |\n",
       "| completeness | ‚úÖ PASS | The answer covers all parts of the user's inquiry, providing a clear explanation of the API's conflict prevention mechanism. |\n",
       "| tool_call_search | ‚úÖ PASS | The agent used the 'text_search' tool to find facts before answering the user's question. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Test test_2: What is the specific comparison method used in the optimistic locking mechanism of the API?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Agent Response:**\n",
       "> The comparison method used in the optimistic locking mechanism of the API is timestamp comparison."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| Check | Result | Justification |\n",
       "| :--- | :--- | :--- |\n",
       "| instructions_follow | ‚úÖ PASS | The agent used the 'text_search' tool to look up information before answering, following rule 1. |\n",
       "| answer_relevant | ‚úÖ PASS | The response directly answers the user's specific question about the comparison method used in the optimistic locking mechanism of the API. |\n",
       "| answer_clear | ‚úÖ PASS | The answer is easy to understand and technically accurate, stating the comparison method as timestamp comparison. |\n",
       "| answer_citations | ‚ûñ SKIP | There are no source files cited in the response, but the answer is based on the search results provided by the 'text_search' tool. |\n",
       "| completeness | ‚úÖ PASS | The answer covers the user's inquiry about the comparison method used in the optimistic locking mechanism of the API. |\n",
       "| tool_call_search | ‚úÖ PASS | The agent used the 'text_search' tool to find facts before answering the user's question. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for res in results:\n",
    "    display(Markdown(f\"### Test {res.get('test_case_id')}: {res.get('question')}\"))\n",
    "    display(Markdown(f\"**Agent Response:**\\n> {res.get('agent_response')}\"))\n",
    "    \n",
    "    checks_md = \"| Check | Result | Justification |\\n| :--- | :--- | :--- |\\n\"\n",
    "    for check in res.get('evaluation', {}).get('checklist', []):\n",
    "        status = \"‚úÖ PASS\" if check.get('check_pass') is True else (\"‚ùå FAIL\" if check.get('check_pass') is False else \"‚ûñ SKIP\")\n",
    "        checks_md += f\"| {check.get('check_name')} | {status} | {check.get('justification')} |\\n\"\n",
    "    \n",
    "    display(Markdown(checks_md))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
